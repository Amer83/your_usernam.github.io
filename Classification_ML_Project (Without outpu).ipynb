{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c20b9e7f",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ac92f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame as df\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import csv\n",
    "import nltk.tokenize \n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt', quiet=True)\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9616692a",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6066a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv = pd.read_csv(r\"C:\\UCD\\Modules\\Summer\\Week 02\\ML_Project\\21206774.csv\",encoding='utf-8', header=0, index_col = False)\n",
    "data_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5c4ab8",
   "metadata": {},
   "source": [
    "### Pre-analysis exploring and cleaning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae790ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first 10 records\n",
    "data_csv.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b81be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last 10 records\n",
    "data_csv.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9aa66b",
   "metadata": {},
   "source": [
    "#### Data exploration and initial organising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f28d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all the columns where the values are null\n",
    "data_csv.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10701a0b",
   "metadata": {},
   "source": [
    "#### fixing the cells with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04274841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the null cells with some values\n",
    "data_csv[\"short_description\"] = data_csv[\"short_description\"].fillna(\"no_description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93caedba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv[\"headline\"] = data_csv[\"headline\"].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d70a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the empty records\n",
    "data_csv[\"category\"] = data_csv[\"category\"].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ef0795",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv[\"date\"] = data_csv[\"date\"].fillna(\"no_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25db040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv[\"authors\"] = data_csv[\"authors\"].fillna(\"no_authors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709d25e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_csv[\"link\"] = data_csv[\"link\"].fillna(\"no_link\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc05ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make shure the dataset is fixed and no null values are there any more.\n",
    "\n",
    "data_csv.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1c715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the whole csv to lowercase\n",
    "data_csv = data_csv.applymap(lambda s: s.lower() if type(s) == str else s)\n",
    "\n",
    "## https://stackoverflow.com/questions/39512002/convert-whole-dataframe-from-lower-case-to-upper-case-with-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3515852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a new csv output file\n",
    "data_csv.to_csv('C:/UCD/Modules/Summer/Week 02/ML_Project/outputfile.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc3ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the new dataset\n",
    "newdata_csv = pd.read_csv(r\"C:\\UCD\\Modules\\Summer\\Week 02\\ML_Project\\outputfile.csv\",encoding='utf-8', header=0, index_col = False)\n",
    "newdata_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d121e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all the \"Unnamed\" columns\n",
    "newdata_csv.drop(newdata_csv.filter(regex=\"Unnamed\"),axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84982fac",
   "metadata": {},
   "source": [
    "#### I manually removed the extra unnamed columns and gave the name \"index\" to the first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53402707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the the remaining null values through the whole dataset\n",
    "newdata_csv.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6f598f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify the categories\n",
    "newdata_csv[\"headline\"] = newdata_csv[\"headline\"].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df64c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata_csv.dropna(subset=['headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1b5a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata_csv['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9c4fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the category data\n",
    "sns.countplot(newdata_csv.category)\n",
    "## https://seaborn.pydata.org/generated/seaborn.countplot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6c809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creat additional column with the length for each headline \n",
    "#and short description columns \n",
    "   \n",
    "\n",
    "newdata_csv ['hd_length']= newdata_csv['headline'].str.len()\n",
    "newdata_csv ['short_des']= newdata_csv['short_description'].str.len()\n",
    "print (newdata_csv ['hd_length'])\n",
    "print (newdata_csv ['short_des'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fae8069",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abda5a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the headline distribution\n",
    "sns.distplot(newdata_csv['hd_length']).set_title('Headlines Distribution')\n",
    "\n",
    "## https://seaborn.pydata.org/generated/seaborn.distplot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba027d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the headline distribution\n",
    "sns.distplot(newdata_csv['short_des']).set_title('Short Description Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1863f5",
   "metadata": {},
   "source": [
    "#### From the readings of the up-generated visualizations, I suspect that there is some mixing between the headlines and the short description in the dataset, which refers to the missing data from \"short decription\" column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5741d1",
   "metadata": {},
   "source": [
    "### NLT process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d3161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://stackoverflow.com/questions/39782418/remove-punctuations-in-pandas\n",
    "\n",
    "# Recognise and remove the punctuation\n",
    "newdata_csv['headline'].str.replace('[^\\w\\s]','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7364dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the headline column\n",
    "headlines = newdata_csv['headline']\n",
    "headlines = [nltk.word_tokenize(headline) for headline in headlines]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819055a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the stopwords from the headline column, and untkenize the words\n",
    "## https://www.datasnips.com/58/remove-stop-words-from-text-in-dataframe-column/\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "newdata_csv['headline'] = newdata_csv['headline'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b51e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning the tokenized headline words to anew column \"tokenized_headlines\"\n",
    "newdata_csv ['tokenized_headlines'] = newdata_csv ['headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8cd065",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdata_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7388466f",
   "metadata": {},
   "source": [
    "#### Notes: I replaced the null values in all the columns with \"no_headlines, No_short_desc,....\".\n",
    "#### After exploring and analyzing the data, I decided to work on the \"headline\" column since therer is only 14 headlines missing, while in the  \"short decription\" column is having 662 missing rows (around 9% of the dataset), so I decided not to depend on this column in this analysis. \n",
    "#### Then cleaned the data and removed all the punctuation and unnecessary words (stop words) and assigned the cleaned data to a new column \"tokenized_headlines\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eab4668",
   "metadata": {},
   "source": [
    "# Data preperation & modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b14ab89",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51859909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying label encoding\n",
    "\n",
    "from sklearn import preprocessing\n",
    "encoder = preprocessing.LabelEncoder()\n",
    "newdata_csv['category_encoded'] = encoder.fit_transform(newdata_csv['category'])\n",
    "newdata_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd82f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the preprocessed file to avoid executing the whole process in case we closed Jupyter\n",
    "# So we can start from the loading point of the preprocessed file. \n",
    "newdata_csv.to_csv('C:/UCD/Modules/Summer/Week 02/ML_Project/preprocessed_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff83060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the preprocessed_csv as a starting point\n",
    "\n",
    "preprocessed_csv = pd.read_csv(r\"C:\\UCD\\Modules\\Summer\\Week 02\\ML_Project\\preprocessed_data.csv\",encoding='utf-8', header=0)\n",
    "preprocessed_csv.drop(preprocessed_csv.filter(regex=\"Unnamed\"),axis=1, inplace=True)\n",
    "preprocessed_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e221be85",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30674da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_csv['category_encoded'].value_counts()\n",
    "# 0 is for the titles with no category, 1 is for the travel news category, 2 is for the weird news"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460ae688",
   "metadata": {},
   "source": [
    "### Splitting and training the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadde517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = preprocessed_csv['tokenized_headlines'] # The data\n",
    "y = preprocessed_csv['category_encoded'] #The target\n",
    "\n",
    "X_train_plus_valid, X_test, y_train_plus_valid, y_test = train_test_split(X, y, random_state=0, test_size = 0.3, train_size = 0.7)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_plus_valid, y_train_plus_valid, random_state=0, test_size = 0.199/0.7, train_size = 0.5/0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f89acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X_train, y_train\n",
    "valid = X_valid, y_valid\n",
    "test = X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a40e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588cfc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0e4d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"X_test shape: {}\".format(X_test.shape))\n",
    "print(\"y_train shape: {}\".format(y_train.shape))\n",
    "print(\"y_test shape: {}\".format(y_test.shape))\n",
    "print(\"X_valid shape: {}\".format(y_train.shape))\n",
    "print(\"y valid shape: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65449547",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessed_csv.iloc[:,1:65]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a56108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da714f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#split the data into train valid and test set\n",
    "\n",
    "#save the data\n",
    "train.to_csv('train.csv',index=False)\n",
    "valid.to_csv('valid.csv',index=False)\n",
    "test.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea496f04",
   "metadata": {},
   "source": [
    "#### Loading (training and validation csv files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884d8269",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression #A variant regression for classification tasks!\n",
    "from sklearn.naive_bayes import GaussianNB as NaiveBayes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97014c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0be0cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_csv = pd.read_csv('valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e543e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b269c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AS we are working with text data, we will need to convert that text into numerical form\n",
    "# I will use tfidf victorizer to do that task\n",
    "# first we will need the following parameters for this task\n",
    "\n",
    "\n",
    "# Parameters for the tfidf\n",
    "ngram_range = (1, 2)\n",
    "min_df = 10\n",
    "max_df = 1.\n",
    "max_features = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2fca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf victorizer\n",
    "# Tomap the most frequent words and compute their occurance\n",
    "vectorizer = TfidfVectorizer(encoding ='utf-8', ngram_range = ngram_range, \n",
    "                         lowercase = False, \n",
    "                        max_df = max_df, min_df = min_df,\n",
    "                        max_features = max_features, norm = 'l2',# for normalization,\n",
    "                        stop_words=None, sublinear_tf = True)\n",
    "\n",
    "# Store all of the training data features in variables (to use them in the ML algorithms)\n",
    "feature_store_train = vectorizer.fit_transform (X_train).toarray()\n",
    "labels_train = y_train\n",
    "\n",
    "#Store all of the validation data features\n",
    "feature_store_valid = vectorizer.transform (X_valid).toarray()\n",
    "labels_valid = y_valid\n",
    "\n",
    "feature_store_test = vectorizer.transform (X_test).toarray()\n",
    "labels_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a61f8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train.shape # X train value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd854abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_valid.shape # X valid value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03abda05",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test.shape # X test value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9103c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_train.shape # y train value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f95f1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_valid.shape # y valid value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5118305",
   "metadata": {},
   "source": [
    "### Building classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103ab079",
   "metadata": {},
   "source": [
    "**I will be using two clssifiers, LogisticRegression and KNeighbors as they can easily deal with text classification and match the text to the category it beelongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa2624f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Applying LogisticRegression classifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "lg = LogisticRegression()\n",
    "lg.fit(feature_store_train, labels_train)\n",
    "\n",
    "# Predict and evaluate the model using the accuracy metric\n",
    "model_predictions = lg.predict(feature_store_test)\n",
    "print('Accuracy', accuracy_score(labels_test, model_predictions))\n",
    "print(metrics.classification_report(labels_test,model_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dbf326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying KNeighbors classifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNeighbors = KNeighborsClassifier()\n",
    "KNeighbors.fit(feature_store_train, labels_train)\n",
    "\n",
    "# Predict and evaluate the model using the accuracy metric\n",
    "model_predictions = KNeighbors.predict(feature_store_test)\n",
    "print('Accuracy', accuracy_score(labels_test, model_predictions))\n",
    "print(metrics.classification_report(labels_test,model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc428a26",
   "metadata": {},
   "source": [
    "**Findings:\n",
    "-LogisticsRegression classifier (lg) \n",
    "As we can notice the scores are good, between 0.86 and 0.98 ,however the category 1 achieved averagely relevant recall score (0.50) and the same for the f1 score (0.64).\n",
    "-As for Kneighbors classifier (KNeighbors)\n",
    "The accuracy score was almost the same as lg (0.81), and the other scores generally are good, however the scores of the category 1 which are relatively average 0.65 for precision, 0.54 for recall, and score of 0.59 for f1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaf17d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation LogisticRegression using the confusion matrix\n",
    "# I chose the confusion matrix to make sure the results are ok\n",
    "\n",
    "X_cm = feature_store_train\n",
    "y_true_labels = labels_train\n",
    "model = lg\n",
    "\n",
    "y_pred = model.predict(X_cm)\n",
    "print(metrics.classification_report(y_true_labels, y_pred))\n",
    "\n",
    "cm=confusion_matrix(y_true_labels, y_pred)\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax); \n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faed359a",
   "metadata": {},
   "source": [
    "**The confusion matrix generated scores for lg shows almost the same ones of the same as the accuracy metric that I used in the LogisticRegression's classifier for the both categories 0 and 1 and that confirms the accuracy of the scores that I got in the lg classifier. \n",
    "\n",
    "**the plot is summerizing the values as following :TP score is 2934, TN is 538, FP is 51, FN is 459, FP is low which is a good indication, while the FN is a bit high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1552cd86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation KNeighbors using the confusion matrix\n",
    "X_cm = feature_store_train\n",
    "y_true_labels = labels_train\n",
    "model = KNeighbors\n",
    "\n",
    "y_pred = model.predict(X_cm)\n",
    "print(metrics.classification_report(y_true_labels, y_pred))\n",
    "\n",
    "cm=confusion_matrix(y_true_labels, y_pred)\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax); \n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11feeb23",
   "metadata": {},
   "source": [
    "**We note a little improvement in the scores in general comparing to the accuracy metric that I used in the KNeighbors classifier. \n",
    "the plot is summerizing the values as following :TP score is 2808, TN is 619, FP is 177, FN is 378, FP is a bit low which is a good indication, while the FN is a bit high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a3f5dd",
   "metadata": {},
   "source": [
    "### Evaluation of the performance of the models on the validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51aaf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying LogisticRegression classifier on validation set \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "lg = LogisticRegression()\n",
    "lg.fit(feature_store_valid, labels_valid)\n",
    "\n",
    "# Predict and evaluate the model using the accuracy metric\n",
    "model_predictions = lg.predict(feature_store_test)\n",
    "print('Accuracy', accuracy_score(labels_test, model_predictions))\n",
    "print(metrics.classification_report(labels_test,model_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b13aa3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Evaluating confusion matrix on the validation set (LogisticsRegression)\n",
    "\n",
    "X_cm = feature_store_valid\n",
    "y_true_labels = labels_valid\n",
    "model = lg\n",
    "\n",
    "y_pred = model.predict(X_cm)\n",
    "print(metrics.classification_report(y_true_labels, y_pred))\n",
    "\n",
    "cm=confusion_matrix(y_true_labels, y_pred)\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax); \n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64906bd",
   "metadata": {},
   "source": [
    "**Applying LogisticRegression classifier on validation set shows good result generally, however the recall score for the category 1 is very low 0.39 and the precision is high 0.89, but with the confusion matrix the results get better (the precision is 95 and the recall is 0.51) however the accuracy scores for both the accuracy metric and the confusion matrix are quite similar.\n",
    "\n",
    "**the plot is summerizing the values as following :TP score is 1166, TN is 208, FP is 12, FN is 200. FP is low which is a good indication, while the FN is a bit high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeb0e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying KNeighbors classifier with the validation dataset\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNeighbors = KNeighborsClassifier()\n",
    "KNeighbors.fit(feature_store_valid, labels_valid)\n",
    "\n",
    "# Predict and evaluate the model using the accuracy metric\n",
    "model_predictions = KNeighbors.predict(feature_store_test)\n",
    "print('Accuracy', accuracy_score(labels_test, model_predictions))\n",
    "print(metrics.classification_report(labels_test,model_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78130bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating confusion matrix on the validation set (LogisticsRegression)\n",
    "\n",
    "X_cm = feature_store_valid\n",
    "y_true_labels = labels_valid\n",
    "model = KNeighbors\n",
    "\n",
    "y_pred = model.predict(X_cm)\n",
    "print(metrics.classification_report(y_true_labels, y_pred))\n",
    "\n",
    "cm=confusion_matrix(y_true_labels, y_pred)\n",
    "ax= plt.subplot()   \n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax); \n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1d0ebc",
   "metadata": {},
   "source": [
    "**The results for the KNeighbors validation are almost the same with the both the accuracy metric and the confusion matrix are almost similar similar however the results becom a bit better after running the confusion matrix.\n",
    "\n",
    "**the plot is summerizing the values as following :TP score is 1124, TN is 234, FP is 54, FN is 174. FP is low which is a good indication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238b0ffd",
   "metadata": {},
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad37e688",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LogisticRegression misclassification rate\n",
    "# I will be using the error rate equasion based on the values of the confusion matrix\n",
    "# that I ran on each model \n",
    "## https://www.ritchieng.com/machine-learning-evaluate-classification-model/\n",
    "\n",
    "FP = 51\n",
    "FN = 459\n",
    "TP = 2934\n",
    "TN = 538\n",
    "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
    "\n",
    "print(classification_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b217c385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kneighbors misclassification rate\n",
    "FP = 177\n",
    "FN = 378\n",
    "TP = 2808\n",
    "TN = 619\n",
    "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
    "\n",
    "print(classification_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02d737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression on validation misclassification rate\n",
    "\n",
    "FP = 12\n",
    "FN = 200\n",
    "TP = 1166\n",
    "TN = 208\n",
    "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
    "\n",
    "print(classification_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e36eea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kneighbors on validation misclassification rate\n",
    "\n",
    "FP = 54\n",
    "FN = 179\n",
    "TP = 1124\n",
    "TN = 234\n",
    "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
    "\n",
    "print(classification_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7b112b",
   "metadata": {},
   "source": [
    "### Applying changes to test the classifiers\n",
    "\n",
    "##### LogisticsRegression classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56d4d18",
   "metadata": {},
   "source": [
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f026e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.youtube.com/watch?v=nFna2s244vA&ab_channel=SolveBusinessProblemsUsingAnalytics\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params_grid = {'C':[0.1, 0.001, 1], 'penalty' : ['l1','l2']} #applying penalty in order to reduce overfitting\n",
    "model = LogisticRegression()\n",
    "clf = GridSearchCV(model, params_grid, cv = 3, verbose = 1)\n",
    "bestF = clf.fit (feature_store_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac815335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the best parameters out of the hyperparameter\n",
    "\n",
    "bestF.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31cc7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying change to the LogisticRegression classifier according to the best parameter extracted\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "lg = LogisticRegression(C=1, penalty = 'l2')\n",
    "lg.fit(feature_store_train, labels_train)\n",
    "\n",
    "# Predict and evaluate the model using the accuracy metric\n",
    "model_predictions = lg.predict(feature_store_test)\n",
    "print('Accuracy', accuracy_score(labels_test, model_predictions))\n",
    "print(metrics.classification_report(labels_test,model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd9b003",
   "metadata": {},
   "source": [
    "By comparing the results of the classifier  with the old ones We can notice that the scores didn't change, no increase or decrease occured, which means that the values are the best we can acheive from the LogisticRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafd3b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation the model using the confusion matrix to make sure nothing has changed.\n",
    "\n",
    "X_cm = feature_store_train\n",
    "y_true_labels = labels_train\n",
    "model = lg\n",
    "\n",
    "y_pred = model.predict(X_cm)\n",
    "print(metrics.classification_report(y_true_labels, y_pred))\n",
    "\n",
    "cm=confusion_matrix(y_true_labels, y_pred)\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax); \n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbb4037",
   "metadata": {},
   "source": [
    "No values has changed with confusion matrix as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c5f5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the error rate on the LogisticRegression\n",
    "FP = 51\n",
    "FN = 459\n",
    "TP = 2934\n",
    "TN = 538\n",
    "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
    "\n",
    "print(classification_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689aabcb",
   "metadata": {},
   "source": [
    "Same as before with the error rate the results are the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f877d84",
   "metadata": {},
   "source": [
    "##### Kneighbors classification model\n",
    "\n",
    "#### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbe153b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeating the same steps but this time for Kneighbors parameters\n",
    "\n",
    "params_KNN = {'n_neighbors': [1,2,3,4,5,6,7], 'p':[1,2,5]} #applying penalty in order to reduce overfitting\n",
    "KN_model = KNeighborsClassifier()\n",
    "clf = GridSearchCV(KN_model, params_KNN, cv=3, verbose = 1, n_jobs = -1)\n",
    "bestF = clf.fit (feature_store_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ce11c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the best parameters out of the hyperparameter\n",
    "\n",
    "bestF.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05194620",
   "metadata": {},
   "outputs": [],
   "source": [
    "KN = KNeighborsClassifier(n_neighbors = 6, p = 2)\n",
    "KN.fit(feature_store_train, labels_train)\n",
    "\n",
    "# Predict and evaluate the model using the accuracy metric\n",
    "model_predictions = KN.predict(feature_store_test)\n",
    "print('Accuracy', accuracy_score(labels_test, model_predictions))\n",
    "print(metrics.classification_report(labels_test,model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715c4de2",
   "metadata": {},
   "source": [
    "Slight improvement acheived regarding the accuracy score (was 0.81 and became 0.83), the recall for catagory 0 became 0.95 (was 0.91), the f1 score now is 0.90 (was 0.88). \n",
    "The precision score for the catagory 1 became 0.76 (was 0.65) while the recall became 0.46 (was 0.54), and the f1 score now is 0.57 (was 0.59) which is recording a slight decrease in these values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f7858e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating KNeighbors using the confusion matrix after the new results\n",
    "\n",
    "X_cm = feature_store_train\n",
    "y_true_labels = labels_train\n",
    "model = KN\n",
    "\n",
    "y_pred = model.predict(X_cm)\n",
    "print(metrics.classification_report(y_true_labels, y_pred))\n",
    "\n",
    "cm=confusion_matrix(y_true_labels, y_pred)\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax); \n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff219c86",
   "metadata": {},
   "source": [
    "FP is 101 (was 177), TP is 2884 (was 2934), FN is 480 (was 459), TN is 517 (was 538.\n",
    "A very slight increase with the values acheived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af1ce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kneighbors misclassification rate\n",
    "FP = 101\n",
    "FN = 480\n",
    "TP = 2884\n",
    "TN = 517\n",
    "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
    "\n",
    "print(classification_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a04678",
   "metadata": {},
   "source": [
    "The rate was 0.13 and became 0.14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d048f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the logisticRegression model using pickle\n",
    "## https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/\n",
    "## https://www.youtube.com/watch?v=KfnhNlD8WZI&ab_channel=codebasics\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open ('LogisticRegression_model', 'wb') as lgmodel:\n",
    "    pickle.dump (lg, lgmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec4496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the KNeighbors model using pickle\n",
    "import pickle\n",
    "\n",
    "with open ('KNeighbors_model', 'wb') as KNeighborsmodel:\n",
    "    pickle.dump (KN, KNeighborsmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60350bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the logisticRegression model (the best model)\n",
    "\n",
    "with open ('LogisticRegression_model', 'rb') as lgmodel:\n",
    "    mp_lg_pkl = pickle.load (lgmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e402c49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the logisticRegression model (the best model)\n",
    "\n",
    "with open ('KNeighbors_model', 'rb') as KNeighborsmodel:\n",
    "    mp_kn_pkl = pickle.load (KNeighborsmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034f21b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging train and valid data sets\n",
    "## https://www.codegrepper.com/code-examples/python/How+to+Merge+train+and+Test+dataset+in+python\n",
    "\n",
    "merged_dset=train_csv.append(valid_csv)\n",
    "merged_dset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c551e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f25025",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dset.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edee801a",
   "metadata": {},
   "source": [
    "### Performing cross validation on the merged data sets (merged_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c2316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset\n",
    "X = merged_dset.loc[:,\"tokenized_headlines\"] # The data\n",
    "y = merged_dset.loc[:,\"category_encoded\"] # the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4765d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_plus_valid, X_test, y_train_plus_valid, y_test = train_test_split(X, y, random_state=0, test_size = 0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4427f092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Victorize and store all the data features\n",
    "\n",
    "feature_train = vectorizer.fit_transform (X_train).toarray()\n",
    "lab_train = y_train\n",
    "feature_test = vectorizer.fit_transform (X_test).toarray()\n",
    "lab_test = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0c6de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b16ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eda68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "kf = KFold(n_splits=5)\n",
    "kf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de1f3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the cross validation on logisticsRegression saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3669b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_lg_pkl.fit(feature_train, lab_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6272a4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://www.youtube.com/watch?v=gJo0uNL-5Qw&t=871s&ab_channel=codebasics\n",
    "cross_val_score(mp_lg_pkl, feature_train, lab_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc79be39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the cross validation on KNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f769346",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_kn_pkl.fit(feature_train, lab_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a6032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(mp_kn_pkl , feature_train, lab_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0479a3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WE can notice that the accuracy results of the KNeighbors model are lower than the ones of the logistics regression's \n",
    "# which also confirm's that the logistics regression is the best model for this dataset, the logistics regression achieved 0.87 before cross validation\n",
    "# and after cross validation the result became 0.86, while the result of the KNeighbors was 0.85 and became 0.82 after cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4805bed0",
   "metadata": {},
   "source": [
    "### Applying the best model to the test.csv data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df8bfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the test dataset\n",
    "test_csv = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674fb3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da519453",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv['category_encoded'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5198ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee74c055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gettiing rid of the values with no meaning\n",
    "\n",
    "test_csv[test_csv[\"headline\"].str.contains(\"no_headline\")==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57074e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all the 0 values from the \"category_encoded\" column\n",
    "# I noticed that sometimes around 10 of the 0 values show up for the empty rows that I filled with some data to avoid keeping them empty. \n",
    "test_csv = test_csv[test_csv.category_encoded != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bffdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I already did the data cleaning on the file so I don't need to do it again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708dc0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a404253",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc45dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the data set is not containing any null values\n",
    "test_csv.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebb8d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_store_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4211e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e952b301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the LogisticRegression model (from cross validation) saved model to the test.csv file\n",
    "\n",
    "mp_lg_pkl.fit(feature_store_test, labels_test)\n",
    "model_predictions = mp_lg_pkl.predict(feature_store_test)\n",
    "print('Accuracy', accuracy_score(labels_test, model_predictions))\n",
    "print(metrics.classification_report(labels_test,model_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5486112b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the LogisticRegression saved model to the test.csv set\n",
    "X_cm = feature_store_test\n",
    "y_true_labels = labels_test\n",
    "model = mp_lg_pkl\n",
    "\n",
    "y_pred = model.predict(X_cm)\n",
    "print(metrics.classification_report(y_true_labels, y_pred))\n",
    "\n",
    "cm=confusion_matrix(y_true_labels, y_pred)\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax); \n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94563390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prediction model's (accuracy, recall,and the f1) results are quite similar to the validation ones, the only very slight differences were in the precision and recall (with a difference of 0.01 only)\n",
    "# The confusion matrix results for validation were 0.87 for accuracy while the test's accuracy is 0.84, the recall and f1 results are a bit different for the validation set/category 1 (0.51 and 0.66 respectively) while they are 0.39 and 0.54 for the test set,\n",
    "# which means the algorithm is not returniing enough of relevant results.\n",
    "# however, the preceision is almost the same 0.95 and 0.90.which are still high which is good indication,\n",
    "# it's returning more relevant resulta than the irrelevant ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee0db46",
   "metadata": {},
   "source": [
    "### Applying the best classifier to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffb12e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "lg = LogisticRegression(C=1, penalty = 'l2')\n",
    "lg.fit(feature_store_test, labels_test)\n",
    "\n",
    "# Predict and evaluate the model using the accuracy metric\n",
    "model_predictions = lg.predict(feature_store_test)\n",
    "print('Accuracy', accuracy_score(labels_test, model_predictions))\n",
    "print(metrics.classification_report(labels_test,model_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d7890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the confusion matrix on the test set\n",
    "\n",
    "X_cm = feature_store_test\n",
    "y_true_labels = labels_test\n",
    "model = lg\n",
    "\n",
    "y_pred = model.predict(X_cm)\n",
    "print(metrics.classification_report(y_true_labels, y_pred))\n",
    "\n",
    "cm=confusion_matrix(y_true_labels, y_pred)\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax); \n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix'); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd72faba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After applying the best model on the test set, it produced exactly the same results of the validation\n",
    "# so no difference has happend. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
